{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 16:54:02.888754: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-10 16:54:03.627037: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-10 16:54:03.629587: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-10 16:54:07.080628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from power.params import *\n",
    "from power.ml_ops.data import get_data_with_cache, get_forecast_data, clean_forecast_data\n",
    "from power.ml_ops.cross_val import get_folds, train_test_split, get_X_y_seq\n",
    "from power.ml_ops.model import init_baseline_keras, compile_model, initialize_model, train_model\n",
    "from power.utils import compress\n",
    "\n",
    "# tensforflow\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../raw_data/history_forecast_bulk_20171007_20240312.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Reduces size of dataframe by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum()/ 1024\n",
    "    print(\"new dataframe size: \", round(input_size,2), 'kB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    for type in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=type))\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=type)\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new dataframe size: \", round(out_size / 1024,2), \" kB\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = compress(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data.copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['forecast dt iso', 'slice dt iso', 'temperature', 'dew_point', 'pressure',\n",
    "       'ground_pressure', 'humidity', 'clouds', 'wind_speed', 'wind_deg',\n",
    "       'rain', 'snow', 'ice', 'fr_rain', 'convective', 'snow_depth',\n",
    "       'accumulated', 'hours', 'rate', 'probability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['forecast dt iso'] = df['forecast dt iso'].str.replace('+0000 UTC', '')\n",
    "df['slice dt iso'] = df['slice dt iso'].str.replace('+0000 UTC', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['forecast dt iso'].str.contains('12:00:00')]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['forecast dt iso'] = pd.to_datetime(df['forecast dt iso'])\n",
    "df['slice dt iso'] = pd.to_datetime(df['slice dt iso'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_dates = df['forecast dt iso'].unique()\n",
    "df_unique_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['forecast dt iso'] == df_unique_dates[0]) & (df['slice dt iso'].between(df_unique_dates[0] + timedelta(days=1) - timedelta(hours=12), df_unique_dates[0] + timedelta(days=1) + timedelta(hours=11)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revised = []\n",
    "for date in df_unique_dates:\n",
    "    data = df[(df['forecast dt iso'] == date) & (df['slice dt iso'].between(date + timedelta(days=1) - timedelta(hours=12), date + timedelta(days=2) + timedelta(hours=11)))]\n",
    "    df_revised.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revised_ordered = pd.concat(df_revised, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revised_ordered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = df_revised_ordered[df_revised_ordered['slice dt iso'] <= '2022-12-31 23:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_forecast_data(forecast_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Initial has 3.3 M entries (everyday: 4 forecasts of 16 days ahead)\n",
    "    Cleaning it to: - 1 forecast perday (at 12:00)\n",
    "                    - 48 hours a day\n",
    "                    - right now hardcoded to match last forecast day with\n",
    "                     last day of PV data\n",
    "    \"\"\"\n",
    "    df = compress(forecast_df)\n",
    "\n",
    "    # get only 1 forecast per day and deal with uncommon UTC format\n",
    "    df['forecast dt iso'] = df['forecast dt iso'].str.replace('+0000 UTC', '')\n",
    "    df['slice dt iso'] = df['slice dt iso'].str.replace('+0000 UTC', '')\n",
    "\n",
    "    df = df[df['forecast dt iso'].str.contains('12:00:00')]\n",
    "\n",
    "    df['forecast dt iso'] = pd.to_datetime(df['forecast dt iso'])\n",
    "    df['slice dt iso'] = pd.to_datetime(df['slice dt iso'])\n",
    "\n",
    "    df_unique_dates = df['forecast dt iso'].unique()\n",
    "\n",
    "    # reduce to 48h of weather forecast (from 00:00 to 23:00 each day)\n",
    "    df_revised = []\n",
    "    for date in df_unique_dates:\n",
    "        data = df[(df['forecast dt iso'] == date) & (df['slice dt iso'].between(date + timedelta(days=1) - timedelta(hours=12), date + timedelta(days=2) + timedelta(hours=11)))]\n",
    "        df_revised.append(data)\n",
    "\n",
    "    df_revised_ordered = pd.concat(df_revised, ignore_index=True)\n",
    "\n",
    "    # hard code the end date to match wiht PV data\n",
    "    processed_df = df_revised_ordered[df_revised_ordered['slice dt iso'] <= '2022-12-31 23:00:00']\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# data loaded\n"
     ]
    }
   ],
   "source": [
    "raw_data = get_forecast_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forecast_dt_unixtime</th>\n",
       "      <th>forecast_dt_iso</th>\n",
       "      <th>slice_dt_unixtime</th>\n",
       "      <th>slice_dt_iso</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>temperature</th>\n",
       "      <th>dew_point</th>\n",
       "      <th>pressure</th>\n",
       "      <th>ground_pressure</th>\n",
       "      <th>humidity</th>\n",
       "      <th>clouds</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_deg</th>\n",
       "      <th>rain</th>\n",
       "      <th>snow</th>\n",
       "      <th>ice</th>\n",
       "      <th>fr_rain</th>\n",
       "      <th>convective</th>\n",
       "      <th>snow_depth</th>\n",
       "      <th>accumulated</th>\n",
       "      <th>hours</th>\n",
       "      <th>rate</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1507334400</td>\n",
       "      <td>2017-10-07 00:00:00 +0000 UTC</td>\n",
       "      <td>1507334400</td>\n",
       "      <td>2017-10-07 00:00:00 +0000 UTC</td>\n",
       "      <td>52.47</td>\n",
       "      <td>13.4</td>\n",
       "      <td>9.07</td>\n",
       "      <td>7.75</td>\n",
       "      <td>1015.81</td>\n",
       "      <td>1010.16</td>\n",
       "      <td>91.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.78</td>\n",
       "      <td>278.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1507334400</td>\n",
       "      <td>2017-10-07 00:00:00 +0000 UTC</td>\n",
       "      <td>1507338000</td>\n",
       "      <td>2017-10-07 01:00:00 +0000 UTC</td>\n",
       "      <td>52.47</td>\n",
       "      <td>13.4</td>\n",
       "      <td>9.49</td>\n",
       "      <td>7.62</td>\n",
       "      <td>1015.75</td>\n",
       "      <td>1010.19</td>\n",
       "      <td>88.92</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.64</td>\n",
       "      <td>280.85</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1507334400</td>\n",
       "      <td>2017-10-07 00:00:00 +0000 UTC</td>\n",
       "      <td>1507341600</td>\n",
       "      <td>2017-10-07 02:00:00 +0000 UTC</td>\n",
       "      <td>52.47</td>\n",
       "      <td>13.4</td>\n",
       "      <td>9.64</td>\n",
       "      <td>7.68</td>\n",
       "      <td>1015.80</td>\n",
       "      <td>1010.29</td>\n",
       "      <td>88.41</td>\n",
       "      <td>54.0</td>\n",
       "      <td>5.09</td>\n",
       "      <td>279.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1507334400</td>\n",
       "      <td>2017-10-07 00:00:00 +0000 UTC</td>\n",
       "      <td>1507345200</td>\n",
       "      <td>2017-10-07 03:00:00 +0000 UTC</td>\n",
       "      <td>52.47</td>\n",
       "      <td>13.4</td>\n",
       "      <td>9.63</td>\n",
       "      <td>7.85</td>\n",
       "      <td>1015.91</td>\n",
       "      <td>1010.40</td>\n",
       "      <td>89.11</td>\n",
       "      <td>64.0</td>\n",
       "      <td>5.25</td>\n",
       "      <td>276.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1507334400</td>\n",
       "      <td>2017-10-07 00:00:00 +0000 UTC</td>\n",
       "      <td>1507348800</td>\n",
       "      <td>2017-10-07 04:00:00 +0000 UTC</td>\n",
       "      <td>52.47</td>\n",
       "      <td>13.4</td>\n",
       "      <td>9.61</td>\n",
       "      <td>8.02</td>\n",
       "      <td>1016.03</td>\n",
       "      <td>1010.52</td>\n",
       "      <td>89.87</td>\n",
       "      <td>68.0</td>\n",
       "      <td>5.26</td>\n",
       "      <td>271.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292254</th>\n",
       "      <td>1710266400</td>\n",
       "      <td>2024-03-12 18:00:00 +0000 UTC</td>\n",
       "      <td>1711634400</td>\n",
       "      <td>2024-03-28 14:00:00 +0000 UTC</td>\n",
       "      <td>52.47</td>\n",
       "      <td>13.4</td>\n",
       "      <td>20.75</td>\n",
       "      <td>10.15</td>\n",
       "      <td>1005.67</td>\n",
       "      <td>1000.06</td>\n",
       "      <td>50.23</td>\n",
       "      <td>99.0</td>\n",
       "      <td>3.07</td>\n",
       "      <td>194.66</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292255</th>\n",
       "      <td>1710266400</td>\n",
       "      <td>2024-03-12 18:00:00 +0000 UTC</td>\n",
       "      <td>1711638000</td>\n",
       "      <td>2024-03-28 15:00:00 +0000 UTC</td>\n",
       "      <td>52.47</td>\n",
       "      <td>13.4</td>\n",
       "      <td>20.55</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1004.55</td>\n",
       "      <td>998.95</td>\n",
       "      <td>51.20</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.99</td>\n",
       "      <td>190.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292256</th>\n",
       "      <td>1710266400</td>\n",
       "      <td>2024-03-12 18:00:00 +0000 UTC</td>\n",
       "      <td>1711641600</td>\n",
       "      <td>2024-03-28 16:00:00 +0000 UTC</td>\n",
       "      <td>52.47</td>\n",
       "      <td>13.4</td>\n",
       "      <td>19.93</td>\n",
       "      <td>10.38</td>\n",
       "      <td>1003.55</td>\n",
       "      <td>997.96</td>\n",
       "      <td>54.18</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.81</td>\n",
       "      <td>183.76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292257</th>\n",
       "      <td>1710266400</td>\n",
       "      <td>2024-03-12 18:00:00 +0000 UTC</td>\n",
       "      <td>1711645200</td>\n",
       "      <td>2024-03-28 17:00:00 +0000 UTC</td>\n",
       "      <td>52.47</td>\n",
       "      <td>13.4</td>\n",
       "      <td>18.87</td>\n",
       "      <td>10.75</td>\n",
       "      <td>1002.78</td>\n",
       "      <td>997.18</td>\n",
       "      <td>59.60</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.53</td>\n",
       "      <td>173.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3292258</th>\n",
       "      <td>1710266400</td>\n",
       "      <td>2024-03-12 18:00:00 +0000 UTC</td>\n",
       "      <td>1711648800</td>\n",
       "      <td>2024-03-28 18:00:00 +0000 UTC</td>\n",
       "      <td>52.47</td>\n",
       "      <td>13.4</td>\n",
       "      <td>17.33</td>\n",
       "      <td>11.39</td>\n",
       "      <td>1002.32</td>\n",
       "      <td>996.72</td>\n",
       "      <td>67.90</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.17</td>\n",
       "      <td>159.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3292259 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         forecast_dt_unixtime                forecast_dt_iso  \\\n",
       "0                  1507334400  2017-10-07 00:00:00 +0000 UTC   \n",
       "1                  1507334400  2017-10-07 00:00:00 +0000 UTC   \n",
       "2                  1507334400  2017-10-07 00:00:00 +0000 UTC   \n",
       "3                  1507334400  2017-10-07 00:00:00 +0000 UTC   \n",
       "4                  1507334400  2017-10-07 00:00:00 +0000 UTC   \n",
       "...                       ...                            ...   \n",
       "3292254            1710266400  2024-03-12 18:00:00 +0000 UTC   \n",
       "3292255            1710266400  2024-03-12 18:00:00 +0000 UTC   \n",
       "3292256            1710266400  2024-03-12 18:00:00 +0000 UTC   \n",
       "3292257            1710266400  2024-03-12 18:00:00 +0000 UTC   \n",
       "3292258            1710266400  2024-03-12 18:00:00 +0000 UTC   \n",
       "\n",
       "         slice_dt_unixtime                   slice_dt_iso    lat   lon  \\\n",
       "0               1507334400  2017-10-07 00:00:00 +0000 UTC  52.47  13.4   \n",
       "1               1507338000  2017-10-07 01:00:00 +0000 UTC  52.47  13.4   \n",
       "2               1507341600  2017-10-07 02:00:00 +0000 UTC  52.47  13.4   \n",
       "3               1507345200  2017-10-07 03:00:00 +0000 UTC  52.47  13.4   \n",
       "4               1507348800  2017-10-07 04:00:00 +0000 UTC  52.47  13.4   \n",
       "...                    ...                            ...    ...   ...   \n",
       "3292254         1711634400  2024-03-28 14:00:00 +0000 UTC  52.47  13.4   \n",
       "3292255         1711638000  2024-03-28 15:00:00 +0000 UTC  52.47  13.4   \n",
       "3292256         1711641600  2024-03-28 16:00:00 +0000 UTC  52.47  13.4   \n",
       "3292257         1711645200  2024-03-28 17:00:00 +0000 UTC  52.47  13.4   \n",
       "3292258         1711648800  2024-03-28 18:00:00 +0000 UTC  52.47  13.4   \n",
       "\n",
       "         temperature  dew_point  pressure  ground_pressure  humidity  clouds  \\\n",
       "0               9.07       7.75   1015.81          1010.16     91.80     0.0   \n",
       "1               9.49       7.62   1015.75          1010.19     88.92    33.0   \n",
       "2               9.64       7.68   1015.80          1010.29     88.41    54.0   \n",
       "3               9.63       7.85   1015.91          1010.40     89.11    64.0   \n",
       "4               9.61       8.02   1016.03          1010.52     89.87    68.0   \n",
       "...              ...        ...       ...              ...       ...     ...   \n",
       "3292254        20.75      10.15   1005.67          1000.06     50.23    99.0   \n",
       "3292255        20.55      10.20   1004.55           998.95     51.20   100.0   \n",
       "3292256        19.93      10.38   1003.55           997.96     54.18   100.0   \n",
       "3292257        18.87      10.75   1002.78           997.18     59.60   100.0   \n",
       "3292258        17.33      11.39   1002.32           996.72     67.90   100.0   \n",
       "\n",
       "         wind_speed  wind_deg  rain  snow  ice  fr_rain  convective  \\\n",
       "0              3.78    278.11   0.0   0.0  0.0      0.0       0.000   \n",
       "1              4.64    280.85   0.0   0.0  0.0      0.0       0.000   \n",
       "2              5.09    279.80   0.0   0.0  0.0      0.0       0.000   \n",
       "3              5.25    276.33   0.0   0.0  0.0      0.0       0.000   \n",
       "4              5.26    271.81   0.0   0.0  0.0      0.0       0.000   \n",
       "...             ...       ...   ...   ...  ...      ...         ...   \n",
       "3292254        3.07    194.66   0.0   0.0  0.0      0.0       1.479   \n",
       "3292255        2.99    190.65   0.0   0.0  0.0      0.0       1.479   \n",
       "3292256        2.81    183.76   0.0   0.0  0.0      0.0       1.479   \n",
       "3292257        2.53    173.43   0.0   0.0  0.0      0.0       1.479   \n",
       "3292258        2.17    159.12   0.0   0.0  0.0      0.0       1.500   \n",
       "\n",
       "         snow_depth  accumulated  hours      rate  probability  \n",
       "0               0.0          0.0    1.0  0.000000         0.00  \n",
       "1               0.0          0.0    1.0  0.000000         0.04  \n",
       "2               0.0          0.0    1.0  0.000000         0.03  \n",
       "3               0.0          0.0    1.0  0.000000         0.00  \n",
       "4               0.0          0.0    1.0  0.000008         0.00  \n",
       "...             ...          ...    ...       ...          ...  \n",
       "3292254         0.0          0.0    1.0  0.000000         0.00  \n",
       "3292255         0.0          0.0    1.0  0.000000         0.00  \n",
       "3292256         0.0          0.0    1.0  0.000000         0.02  \n",
       "3292257         0.0          0.0    1.0  0.000003         0.06  \n",
       "3292258         0.0          0.0    1.0  0.000006         0.13  \n",
       "\n",
       "[3292259 rows x 24 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "raw_data.rename(columns={'forecast dt unixtime': 'forecast_dt_unixtime',\n",
    "               'forecast dt iso': 'forecast_dt_iso',\n",
    "               'slice dt unixtime': 'slice_dt_unixtime',\n",
    "               'slice dt iso': 'slice_dt_iso'}, inplace=True)\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = clean_forecast_data(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df.to_csv('../raw_data/weather_forecast_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = pd.read_csv('../raw_data/weather_forecast_processed.csv')\n",
    "pv_weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pv_weather_df.copy()\n",
    "data.rename(columns={'forecast dt iso':'utc_time', 'slice dt iso':'prediction_utc_time'}, inplace=True)\n",
    "data['utc_time'] = pd.to_datetime(data['utc_time'])\n",
    "data['prediction_utc_time'] = pd.to_datetime(data['prediction_utc_time'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_date ='2020-12-06'\n",
    "input_datetime = datetime.strptime(input_date, '%Y-%m-%d')\n",
    "data[data.utc_time.dt.date == (input_datetime.date())].iloc[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_date ='2020-06-30'\n",
    "input_datetime = datetime.strptime(input_date, '%Y-%m-%d')\n",
    "\n",
    "df_forecast_day_before_input_date = data[data.utc_time.dt.date == (input_datetime.date() - timedelta(days=1))].iloc[-24:,:]\n",
    "df_forecast_input_date = data[data.utc_time.dt.date == input_datetime.date()].iloc[:24,:]\n",
    "df_forecast = pd.concat([df_forecast_day_before_input_date, df_forecast_input_date], axis=0).reset_index(drop=True)\n",
    "\n",
    "features = ['temperature', 'clouds', 'wind_deg', 'rain', 'snow',]\n",
    "X=df_forecast[features]\n",
    "fig, ax = plt.subplots(nrows=5, ncols=2, figsize= (16,9))\n",
    "for idx, feature in enumerate(features):\n",
    "    sns.boxplot(data=X, x=feature, ax=ax[idx,0], legend='auto')\n",
    "    sns.histplot(data=X, x=feature, ax=ax[idx,1], bins =5)\n",
    "# df_forecast.iloc[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast.utc_time.nunique(), df_forecast.prediction_utc_time.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast.utc_time.value_counts(), df_forecast.prediction_utc_time.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= MinMaxScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=2, figsize= (16,9))\n",
    "for idx, feature in enumerate(features[-5:]):\n",
    "    sns.boxplot(data=X, x=feature, ax=ax[idx,0], legend='auto')\n",
    "    sns.histplot(data=X, x=feature, ax=ax[idx,1], bins =5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSEUDO-CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when getting the 10_000 sequences, the dt index should be used to extract 48 observations and x features\n",
    "\n",
    "rnow an ouput of shape (48,1)\n",
    "\n",
    "should become an output (48, 1 + x)\n",
    "\n",
    "call a data module.function to process the weather forecast features\n",
    "\n",
    "concat the data somehow: np.concat? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test get weather feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = pd.read_csv('../raw_data/weather_forecast_processed.csv')\n",
    "\n",
    "def compress(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Reduces size of dataframe by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum()/ 1024\n",
    "    print(\"new dataframe size: \", round(input_size,2), 'kB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    for type in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=type))\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=type)\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new dataframe size: \", round(out_size / 1024,2), \" kB\")\n",
    "\n",
    "    return df\n",
    "\n",
    "pv_weather_df = compress(pv_weather_df)\n",
    "\n",
    "def get_weather_forecast_features(forecast: pd.DataFrame, input_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    returns the weather forecast data from historical weather forecast in Tempelhof\n",
    "    input: - a processed forecast dataframe of shape (91704,21)\n",
    "           - an input date (str: YYYY-MM-DD)\n",
    "    output: a dataframe of shape (48, 21)\n",
    "            -> first 24 rows: hourly (from 00:00 to 23:00) weather forecast\n",
    "               of input_date +1 forecast on input_date -1 (at 12:00)\n",
    "            -> second 24 rows: hourly (from 00:00 to 23:00) weather forecast\n",
    "               of input_date +1 forecast on input_date (at 12:00)\n",
    "    \"\"\"\n",
    "    forecast.rename(columns={'forecast dt iso':'utc_time',\n",
    "                        'slice dt iso':'prediction_utc_time'},\n",
    "                        inplace=True)\n",
    "    forecast['utc_time'] = pd.to_datetime(forecast['utc_time'])\n",
    "    forecast['prediction_utc_time'] = pd.to_datetime(forecast['prediction_utc_time'])\n",
    "\n",
    "    input_datetime = datetime.strptime(input_date, '%Y-%m-%d')\n",
    "\n",
    "    forecast_day_before_input_date = forecast[forecast.utc_time.dt.date == (input_datetime.date() - timedelta(days=1))].iloc[-24:,:]\n",
    "    forecast_input_date = forecast[forecast.utc_time.dt.date == input_datetime.date()].iloc[:24,:]\n",
    "    df_forecast = pd.concat([forecast_day_before_input_date,\n",
    "                             forecast_input_date], axis=0).reset_index(drop=True)\n",
    "    return df_forecast\n",
    "\n",
    "test_df = get_weather_forecast_features(pv_weather_df, '2020-06-30')\n",
    "test_df.iloc[:,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try editing sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xi_yi(\n",
    "    fold:pd.DataFrame,\n",
    "    input_length:int,       # 48\n",
    "    output_length:int,      # 24\n",
    "    gap_hours):\n",
    "    '''\n",
    "    - given a fold, it returns one sequence (X_i, y_i)\n",
    "    - with the starting point of the sequence being chosen at random\n",
    "    - TARGET is the variable(s) we want to predict (name of the column(s))\n",
    "    '''\n",
    "    TARGET = 'electricity'\n",
    "    first_possible_start = 0\n",
    "    last_possible_start = len(fold) - (input_length + gap_hours + output_length) + 1\n",
    "\n",
    "    random_start = np.random.randint(first_possible_start, last_possible_start)\n",
    "\n",
    "    # input_start & input_end are the indexes of the 48h (training length) of training data\n",
    "    # thus for weather forecast data from input_end should be used to add\n",
    "    # the weather forecast features\n",
    "    input_start = random_start\n",
    "    input_end = random_start + input_length\n",
    "    # here we extract the forecast date and hour\n",
    "    forecast_date = fold.iloc[input_end]['utc_time'].strftime('%Y-%m-%d')\n",
    "    forecast_hour = fold.iloc[input_end]['utc_time'].hour\n",
    "\n",
    "    target_start = input_end + gap_hours\n",
    "    target_end = target_start + output_length\n",
    "\n",
    "    # first we parse the electricity feature\n",
    "    # need to reset index only for X_i in order to be able to concat with X_weather later on\n",
    "    X_i = fold.iloc[input_start:input_end].reset_index()\n",
    "    y_i = fold.iloc[target_start:target_end][[TARGET]]    # creates a pd.DataFrame for the target y\n",
    "\n",
    "    # then we parse/create the weather forecast features\n",
    "    #TODO : proper data\n",
    "    X_weather = get_weather_forecast_features(pv_weather_df, forecast_date)\n",
    "\n",
    "    #\n",
    "    features = ['temperature', 'clouds', 'wind_speed']\n",
    "    to_concat = [X_i['electricity'], X_weather[features]]\n",
    "    X_i = pd.concat(to_concat, axis=1)\n",
    "    return (X_i, y_i)\n",
    "\n",
    "min_date = '2017-10-07 12:00:00'\n",
    "max_date = '2019-12-31 23:00:00'\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "        ORDER BY utc_time\n",
    "    \"\"\"\n",
    "\n",
    "data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "data_processed = get_data_with_cache(\n",
    "    gcp_project=GCP_PROJECT,\n",
    "    query=query,\n",
    "    cache_path=data_processed_cache_path,\n",
    "    data_has_header=True\n",
    ")\n",
    "\n",
    "# the processed data from bq needs to be converted to datetime object\n",
    "data_processed.utc_time = pd.to_datetime(data_processed.utc_time,utc=True)\n",
    "\n",
    "if data_processed.shape[0] < 240:\n",
    "    print(\"❌ Not enough processed data retrieved to train on\")\n",
    "    # return None\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data_processed[(data_processed['utc_time'] > min_date) \\\n",
    "                        & (data_processed['utc_time'] < max_date)]\n",
    "# train = data_processed[data_processed['utc_time'] < max_date]\n",
    "\n",
    "# train = train[['electricity']]\n",
    "\n",
    "fold = train.copy()\n",
    "number_of_sequences=100\n",
    "input_length=48\n",
    "output_length=24\n",
    "gap_hours=12\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(number_of_sequences):\n",
    "    (Xi, yi) = get_Xi_yi(fold, input_length, output_length, gap_hours)\n",
    "    X.append(Xi)\n",
    "    y.append(yi)\n",
    "\n",
    "X_train, y_train = np.array(X), np.array(y)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1:], y_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# For Jerome - ignore the rest ############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = pv_weather_df.rename(columns={'forecast dt iso': 'date_of_forcast',\n",
    "                                              'slice dt iso': 'forcasting_date_range',\n",
    "                                              'fr_rain': 'freezing_rain_vol',\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = pd.merge(pv_weather_df, pv_df, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = '1980-01-01 00:00:00'\n",
    "max_date = '2019-12-31 23:00:00'\n",
    "train = pv_df[pv_df['utc_time'] <= max_date]\n",
    "test = pv_df[pv_df['utc_time'] > max_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['electricity']]\n",
    "test = test[['electricity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'electricity'\n",
    "fold_length = 43800             # 5 years\n",
    "fold_stride = 43800             # 5 years\n",
    "train_test_ratio = 0.8          # 5 yrs/6 yrs\n",
    "input_length = 48               # number of obsevations per one sequence\n",
    "output_length = 24              # Day-ahead predictions\n",
    "n_seq_train = 500               # number_of_sequences_train\n",
    "n_seq_val = 100                 # number_of_sequences_test\n",
    "n_unit = 48                     # number of hidden units\n",
    "learning_rate = 0.02\n",
    "patience = 5\n",
    "epochs = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_baseline_and_lstm():\n",
    "\n",
    "    list_of_mae_baseline_model = []\n",
    "    list_of_mae_recurrent_model = []\n",
    "\n",
    "    # 0 - Creating folds\n",
    "    # =========================================\n",
    "    folds = get_folds(train, fold_length, fold_stride)\n",
    "\n",
    "    for fold_id, fold in enumerate(folds):\n",
    "\n",
    "        # 1 - Train/Test split the current fold\n",
    "        # =========================================\n",
    "        (fold_train, fold_val) = train_test_split(fold, train_test_ratio, input_length)\n",
    "\n",
    "        X_train, y_train = get_X_y_seq(fold_train, n_seq_train, input_length, output_length, gap_hours=12)\n",
    "        X_val, y_val = get_X_y_seq(fold_val, n_seq_val, input_length, output_length, gap_hours=12)\n",
    "\n",
    "        # 2 - Modelling\n",
    "        # =========================================\n",
    "\n",
    "        ##### Baseline Model\n",
    "        baseline_model = init_baseline_keras()\n",
    "        mae_baseline = baseline_model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "        list_of_mae_baseline_model.append(mae_baseline)\n",
    "        print(\"-\"*50)\n",
    "        print(f\"MAE baseline fold n°{fold_id} = {round(mae_baseline, 2)}\")\n",
    "\n",
    "        ##### LSTM Model\n",
    "        model = initialize_model(X_train, y_train, n_unit=n_unit)\n",
    "        model = compile_model(model, learning_rate=learning_rate)\n",
    "        model, history = train_model(model,\n",
    "                                     X_train,\n",
    "                                     y_train,\n",
    "                                     validation_split = 0.2,\n",
    "                                     batch_size = batch_size,\n",
    "                                     epochs = epochs)\n",
    "\n",
    "        # Create a figure and axes object with 1 row and 2 columns\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "        # Plot training & validation MAE values\n",
    "        axes[0].plot(history.history['mae'])\n",
    "        axes[0].plot(history.history['val_mae'])\n",
    "        axes[0].set_title('Model MAE')\n",
    "        axes[0].set_ylabel('MAE')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        axes[1].plot(history.history['loss'])\n",
    "        axes[1].plot(history.history['val_loss'])\n",
    "        axes[1].set_title('Model Loss')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "        res = model.evaluate(X_val, y_val, verbose=0)    # evaluating LSTM (metric)\n",
    "        mae_lstm = res[1]\n",
    "        list_of_mae_recurrent_model.append(mae_lstm)\n",
    "        print(f\"MAE LSTM fold n°{fold_id} = {round(mae_lstm, 2)}\")\n",
    "\n",
    "        ##### Comparison LSTM vs Baseline for the current fold\n",
    "        print(f\"improvement over baseline: {round((1 - (mae_lstm/mae_baseline))*100,2)} % \\n\")\n",
    "\n",
    "    return list_of_mae_baseline_model, list_of_mae_recurrent_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_baselines, mae_lstms = cross_validate_baseline_and_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"average percentage improvement over baseline = {round(np.mean(1 - (np.array(mae_lstms)/np.array(mae_baselines))),2)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y_seq(train, number_of_sequences=10000, input_length=48, output_length=24, gap_hours=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(X_train, y_train, n_unit=n_unit)\n",
    "model = compile_model(model, learning_rate=learning_rate)\n",
    "model, history = train_model(model,\n",
    "                                X_train,\n",
    "                                y_train,\n",
    "                                validation_split = 0.1,\n",
    "                                batch_size = batch_size,\n",
    "                                epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power.params import *\n",
    "from power.ml_ops.data import get_data_with_cache\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "input_pred = '2021-05-08 12:00:00'\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "    ORDER BY utc_time\n",
    "\"\"\"\n",
    "\n",
    "data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "data_processed = get_data_with_cache(\n",
    "    gcp_project=GCP_PROJECT,\n",
    "    query=query,\n",
    "    cache_path=data_processed_cache_path,\n",
    "    data_has_header=True\n",
    ")\n",
    "\n",
    "X_pred = data_processed[data_processed['utc_time'] < input_pred][-48:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = X_pred[['electricity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = X_pred.to_numpy()\n",
    "X_pred_tf = tf.convert_to_tensor(X_pred)\n",
    "X_pred_tf = tf.expand_dims(X_pred_tf, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_pred_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
