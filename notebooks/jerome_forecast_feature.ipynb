{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 08:35:26.318015: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-10 08:35:26.663218: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-10 08:35:26.666340: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-10 08:35:29.241529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from power.params import *\n",
    "from power  .ml_ops.data import get_data_with_cache\n",
    "from power.ml_ops.cross_val import get_folds, train_test_split, get_X_y_seq\n",
    "from power.ml_ops.model import init_baseline_keras, compile_model, initialize_model, train_model\n",
    "\n",
    "# tensforflow\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../raw_data/history_forecast_bulk_20171007_20240312.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Reduces size of dataframe by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum()/ 1024\n",
    "    print(\"new dataframe size: \", round(input_size,2), 'kB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    for type in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=type))\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=type)\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new dataframe size: \", round(out_size / 1024,2), \" kB\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = compress(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['forecast dt iso', 'slice dt iso', 'temperature', 'dew_point', 'pressure',\n",
    "       'ground_pressure', 'humidity', 'clouds', 'wind_speed', 'wind_deg',\n",
    "       'rain', 'snow', 'ice', 'fr_rain', 'convective', 'snow_depth',\n",
    "       'accumulated', 'hours', 'rate', 'probability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['forecast dt iso'] = df['forecast dt iso'].str.replace('+0000 UTC', '')\n",
    "df['slice dt iso'] = df['slice dt iso'].str.replace('+0000 UTC', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['forecast dt iso'].str.contains('12:00:00')]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['forecast dt iso'] = pd.to_datetime(df['forecast dt iso'])\n",
    "df['slice dt iso'] = pd.to_datetime(df['slice dt iso'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_dates = df['forecast dt iso'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['forecast dt iso'] == df_unique_dates[0]) & (df['slice dt iso'].between(df_unique_dates[0] + timedelta(days=1) - timedelta(hours=12), df_unique_dates[0] + timedelta(days=1) + timedelta(hours=11)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revised = []\n",
    "for date in df_unique_dates:\n",
    "    data = df[(df['forecast dt iso'] == date) & (df['slice dt iso'].between(date + timedelta(days=1) - timedelta(hours=12), date + timedelta(days=2) + timedelta(hours=11)))]\n",
    "    df_revised.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revised_ordered = pd.concat(df_revised, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revised_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = df_revised_ordered[df_revised_ordered['slice dt iso'] <= '2022-12-31 23:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df.to_csv('../raw_data/weather_forecast_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = pd.read_csv('../raw_data/weather_forecast_processed.csv')\n",
    "pv_weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pv_weather_df.copy()\n",
    "data.rename(columns={'forecast dt iso':'utc_time', 'slice dt iso':'prediction_utc_time'}, inplace=True)\n",
    "data['utc_time'] = pd.to_datetime(data['utc_time'])\n",
    "data['prediction_utc_time'] = pd.to_datetime(data['prediction_utc_time'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_date ='2020-12-06'\n",
    "input_datetime = datetime.strptime(input_date, '%Y-%m-%d')\n",
    "data[data.utc_time.dt.date == (input_datetime.date())].iloc[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_date ='2020-06-30'\n",
    "input_datetime = datetime.strptime(input_date, '%Y-%m-%d')\n",
    "\n",
    "df_forecast_day_before_input_date = data[data.utc_time.dt.date == (input_datetime.date() - timedelta(days=1))].iloc[-24:,:]\n",
    "df_forecast_input_date = data[data.utc_time.dt.date == input_datetime.date()].iloc[:24,:]\n",
    "df_forecast = pd.concat([df_forecast_day_before_input_date, df_forecast_input_date], axis=0).reset_index(drop=True)\n",
    "\n",
    "features = ['temperature', 'clouds', 'wind_deg', 'rain', 'snow',]\n",
    "X=df_forecast[features]\n",
    "fig, ax = plt.subplots(nrows=5, ncols=2, figsize= (16,9))\n",
    "for idx, feature in enumerate(features):\n",
    "    sns.boxplot(data=X, x=feature, ax=ax[idx,0], legend='auto')\n",
    "    sns.histplot(data=X, x=feature, ax=ax[idx,1], bins =5)\n",
    "# df_forecast.iloc[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast.utc_time.nunique(), df_forecast.prediction_utc_time.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast.utc_time.value_counts(), df_forecast.prediction_utc_time.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= MinMaxScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=2, figsize= (16,9))\n",
    "for idx, feature in enumerate(features[-5:]):\n",
    "    sns.boxplot(data=X, x=feature, ax=ax[idx,0], legend='auto')\n",
    "    sns.histplot(data=X, x=feature, ax=ax[idx,1], bins =5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSEUDO-CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when getting the 10_000 sequences, the dt index should be used to extract 48 observations and x features\n",
    "\n",
    "rnow an ouput of shape (48,1)\n",
    "\n",
    "should become an output (48, 1 + x)\n",
    "\n",
    "call a data module.function to process the weather forecast features\n",
    "\n",
    "concat the data somehow: np.concat? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test get weather feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new dataframe size:  15045.31 kB\n",
      "optimized size by 45.0 %\n",
      "new dataframe size:  8239.16  kB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 91704 entries, 0 to 91703\n",
      "Data columns (total 21 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   Unnamed: 0           91704 non-null  int32         \n",
      " 1   utc_time             91704 non-null  datetime64[ns]\n",
      " 2   prediction_utc_time  91704 non-null  datetime64[ns]\n",
      " 3   temperature          91704 non-null  float32       \n",
      " 4   dew_point            91704 non-null  float32       \n",
      " 5   pressure             91704 non-null  float32       \n",
      " 6   ground_pressure      91704 non-null  float32       \n",
      " 7   humidity             91704 non-null  float32       \n",
      " 8   clouds               91704 non-null  float32       \n",
      " 9   wind_speed           91704 non-null  float32       \n",
      " 10  wind_deg             91704 non-null  float32       \n",
      " 11  rain                 91704 non-null  float32       \n",
      " 12  snow                 91704 non-null  float32       \n",
      " 13  ice                  91704 non-null  float32       \n",
      " 14  fr_rain              91704 non-null  float32       \n",
      " 15  convective           91704 non-null  float32       \n",
      " 16  snow_depth           91704 non-null  float32       \n",
      " 17  accumulated          91704 non-null  float32       \n",
      " 18  hours                91704 non-null  float32       \n",
      " 19  rate                 91704 non-null  float32       \n",
      " 20  probability          91704 non-null  float32       \n",
      "dtypes: datetime64[ns](2), float32(18), int32(1)\n",
      "memory usage: 8.0 MB\n"
     ]
    }
   ],
   "source": [
    "pv_weather_df = pd.read_csv('../raw_data/weather_forecast_processed.csv')\n",
    "\n",
    "def compress(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Reduces size of dataframe by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum()/ 1024\n",
    "    print(\"new dataframe size: \", round(input_size,2), 'kB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    for type in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=type))\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=type)\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new dataframe size: \", round(out_size / 1024,2), \" kB\")\n",
    "\n",
    "    return df\n",
    "\n",
    "pv_weather_df = compress(pv_weather_df)\n",
    "\n",
    "def get_weather_forecast_features(forecast: pd.DataFrame, input_date: str) -> pd.DataFrame:\n",
    "    forecast.rename(columns={'forecast dt iso':'utc_time',\n",
    "                        'slice dt iso':'prediction_utc_time'},\n",
    "                        inplace=True)\n",
    "    forecast['utc_time'] = pd.to_datetime(forecast['utc_time'])\n",
    "    forecast['prediction_utc_time'] = pd.to_datetime(forecast['prediction_utc_time'])\n",
    "\n",
    "    input_datetime = datetime.strptime(input_date, '%Y-%m-%d')\n",
    "\n",
    "    forecast_day_before_input_date = forecast[forecast.utc_time.dt.date == (input_datetime.date() - timedelta(days=1))].iloc[-24:,:]\n",
    "    forecast_input_date = forecast[forecast.utc_time.dt.date == input_datetime.date()].iloc[:24,:]\n",
    "    df_forecast = pd.concat([forecast_day_before_input_date,\n",
    "                             forecast_input_date], axis=0).reset_index(drop=True)\n",
    "    return df_forecast\n",
    "\n",
    "test_df = get_weather_forecast_features(pv_weather_df, '2020-06-30')\n",
    "test_df.iloc[:,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try editing sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\n",
      "Load data from local CSV...\u001b[0m\n",
      "✅ Data loaded, with shape (376944, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((100, 48, 4), (100, 48, 21))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_weather_forecast_features(forecast: pd.DataFrame, input_date: str) -> pd.DataFrame:\n",
    "    forecast.rename(columns={'forecast dt iso':'utc_time',\n",
    "                        'slice dt iso':'prediction_utc_time'},\n",
    "                        inplace=True)\n",
    "    forecast['utc_time'] = pd.to_datetime(forecast['utc_time'])\n",
    "    forecast['prediction_utc_time'] = pd.to_datetime(forecast['prediction_utc_time'])\n",
    "\n",
    "    input_datetime = datetime.strptime(input_date, '%Y-%m-%d')\n",
    "\n",
    "    forecast_day_before_input_date = forecast[forecast.utc_time.dt.date == (input_datetime.date() - timedelta(days=1))].iloc[-24:,:]\n",
    "    forecast_input_date = forecast[forecast.utc_time.dt.date == input_datetime.date()].iloc[:24,:]\n",
    "    df = pd.concat([forecast_day_before_input_date,\n",
    "                             forecast_input_date], axis=0).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_Xi_yi(\n",
    "    fold:pd.DataFrame,\n",
    "    input_length:int,       # 48\n",
    "    output_length:int,      # 24\n",
    "    gap_hours):\n",
    "    '''\n",
    "    - given a fold, it returns one sequence (X_i, y_i)\n",
    "    - with the starting point of the sequence being chosen at random\n",
    "    - TARGET is the variable(s) we want to predict (name of the column(s))\n",
    "    '''\n",
    "    TARGET = 'electricity'\n",
    "    first_possible_start = 0\n",
    "    last_possible_start = len(fold) - (input_length + gap_hours + output_length) + 1\n",
    "\n",
    "    random_start = np.random.randint(first_possible_start, last_possible_start)\n",
    "\n",
    "    input_start = random_start\n",
    "    input_end = random_start + input_length\n",
    "    target_start = input_end + gap_hours\n",
    "    target_end = target_start + output_length\n",
    "\n",
    "    X_i = fold.iloc[input_start:input_end].reset_index()\n",
    "    y_i = fold.iloc[target_start:target_end][[TARGET]]    # creates a pd.DataFrame for the target y\n",
    "\n",
    "    input_date = fold.iloc[target_end]['utc_time'].strftime('%Y-%m-%d')\n",
    "    input_hour = fold.iloc[target_end]['utc_time'].hour\n",
    "\n",
    "    X_weather = get_weather_forecast_features(pv_weather_df, input_date)\n",
    "    features = ['temperature', 'clouds', 'wind_speed']\n",
    "    to_concat = [X_i['electricity'], X_weather[features]]\n",
    "    X_i = pd.concat(to_concat, axis=1)\n",
    "    return (X_i, y_i, X_weather)\n",
    "\n",
    "min_date = '2017-10-07 12:00:00'\n",
    "max_date = '2019-12-31 23:00:00'\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "        ORDER BY utc_time\n",
    "    \"\"\"\n",
    "\n",
    "data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "data_processed = get_data_with_cache(\n",
    "    gcp_project=GCP_PROJECT,\n",
    "    query=query,\n",
    "    cache_path=data_processed_cache_path,\n",
    "    data_has_header=True\n",
    ")\n",
    "\n",
    "# the processed data from bq needs to be converted to datetime object\n",
    "data_processed.utc_time = pd.to_datetime(data_processed.utc_time,utc=True)\n",
    "\n",
    "if data_processed.shape[0] < 240:\n",
    "    print(\"❌ Not enough processed data retrieved to train on\")\n",
    "    # return None\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data_processed[(data_processed['utc_time'] > min_date) \\\n",
    "                        & (data_processed['utc_time'] < max_date)]\n",
    "# train = data_processed[data_processed['utc_time'] < max_date]\n",
    "\n",
    "# train = train[['electricity']]\n",
    "\n",
    "fold = train.copy()\n",
    "number_of_sequences=100\n",
    "input_length=48\n",
    "output_length=24\n",
    "gap_hours=12\n",
    "\n",
    "X, y, X_weather= [], [], []                                              # lists for the sequences for X and y\n",
    "\n",
    "for i in range(number_of_sequences):\n",
    "    (Xi, yi, X_weather_i) = get_Xi_yi(fold, input_length, output_length, gap_hours)   # calls the previous function to generate sequences X + y\n",
    "    X.append(Xi)\n",
    "    y.append(yi)\n",
    "    X_weather.append(X_weather_i)\n",
    "\n",
    "X_train, X_train_weather = np.array(X), np.array(X_weather)\n",
    "X_train.shape, X_train_weather.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.96000000e-01, 1.64400005e+01, 0.00000000e+00, 2.00999999e+00],\n",
       "       [5.79000000e-01, 1.61700001e+01, 0.00000000e+00, 2.06999993e+00],\n",
       "       [3.91000000e-01, 1.59200001e+01, 0.00000000e+00, 2.19000006e+00],\n",
       "       [1.46000000e-01, 1.55500002e+01, 0.00000000e+00, 2.25000000e+00],\n",
       "       [4.00000000e-03, 1.50400000e+01, 0.00000000e+00, 2.17000008e+00],\n",
       "       [0.00000000e+00, 1.46800003e+01, 0.00000000e+00, 2.02999997e+00],\n",
       "       [0.00000000e+00, 1.48500004e+01, 0.00000000e+00, 1.94000006e+00],\n",
       "       [0.00000000e+00, 1.58100004e+01, 0.00000000e+00, 1.97000003e+00],\n",
       "       [0.00000000e+00, 1.73199997e+01, 0.00000000e+00, 2.14000010e+00],\n",
       "       [0.00000000e+00, 1.90400009e+01, 0.00000000e+00, 2.42000008e+00],\n",
       "       [0.00000000e+00, 2.06499996e+01, 0.00000000e+00, 2.77999997e+00],\n",
       "       [0.00000000e+00, 2.19799995e+01, 0.00000000e+00, 3.19000006e+00],\n",
       "       [0.00000000e+00, 2.29099998e+01, 0.00000000e+00, 3.57999992e+00],\n",
       "       [0.00000000e+00, 2.33099995e+01, 0.00000000e+00, 3.89000010e+00],\n",
       "       [0.00000000e+00, 2.31599998e+01, 0.00000000e+00, 4.07000017e+00],\n",
       "       [0.00000000e+00, 2.24500008e+01, 0.00000000e+00, 4.03999996e+00],\n",
       "       [0.00000000e+00, 2.12299995e+01, 0.00000000e+00, 3.77999997e+00],\n",
       "       [0.00000000e+00, 1.98400002e+01, 0.00000000e+00, 3.47000003e+00],\n",
       "       [4.60000000e-02, 1.86800003e+01, 0.00000000e+00, 3.34999990e+00],\n",
       "       [1.66000000e-01, 1.80200005e+01, 0.00000000e+00, 3.57999992e+00],\n",
       "       [3.68000000e-01, 1.76900005e+01, 0.00000000e+00, 4.01000023e+00],\n",
       "       [3.92000000e-01, 1.74300003e+01, 0.00000000e+00, 4.42000008e+00],\n",
       "       [6.08000000e-01, 1.70000000e+01, 0.00000000e+00, 4.63999987e+00],\n",
       "       [6.58000000e-01, 1.64899998e+01, 0.00000000e+00, 4.73999977e+00],\n",
       "       [5.37000000e-01, 1.66299992e+01, 0.00000000e+00, 2.01999998e+00],\n",
       "       [3.53000000e-01, 1.63400002e+01, 0.00000000e+00, 2.06999993e+00],\n",
       "       [2.03000000e-01, 1.60400009e+01, 0.00000000e+00, 2.11999989e+00],\n",
       "       [6.50000000e-02, 1.56499996e+01, 0.00000000e+00, 2.13000011e+00],\n",
       "       [3.00000000e-03, 1.51599998e+01, 0.00000000e+00, 2.06999993e+00],\n",
       "       [0.00000000e+00, 1.48500004e+01, 0.00000000e+00, 2.00999999e+00],\n",
       "       [0.00000000e+00, 1.50699997e+01, 0.00000000e+00, 2.04999995e+00],\n",
       "       [0.00000000e+00, 1.60699997e+01, 0.00000000e+00, 2.25999999e+00],\n",
       "       [0.00000000e+00, 1.76200008e+01, 0.00000000e+00, 2.56999993e+00],\n",
       "       [0.00000000e+00, 1.93600006e+01, 0.00000000e+00, 2.89000010e+00],\n",
       "       [0.00000000e+00, 2.09899998e+01, 0.00000000e+00, 3.15000010e+00],\n",
       "       [0.00000000e+00, 2.23299999e+01, 0.00000000e+00, 3.34999990e+00],\n",
       "       [0.00000000e+00, 2.32500000e+01, 0.00000000e+00, 3.50999999e+00],\n",
       "       [0.00000000e+00, 2.36499996e+01, 0.00000000e+00, 3.65000010e+00],\n",
       "       [0.00000000e+00, 2.34899998e+01, 0.00000000e+00, 3.75000000e+00],\n",
       "       [0.00000000e+00, 2.27700005e+01, 0.00000000e+00, 3.80999994e+00],\n",
       "       [0.00000000e+00, 2.15499992e+01, 0.00000000e+00, 3.79999995e+00],\n",
       "       [0.00000000e+00, 2.01499996e+01, 0.00000000e+00, 3.79999995e+00],\n",
       "       [4.90000000e-02, 1.89500008e+01, 0.00000000e+00, 3.85999990e+00],\n",
       "       [1.55000000e-01, 1.82399998e+01, 0.00000000e+00, 4.01000023e+00],\n",
       "       [2.61000000e-01, 1.78600006e+01, 0.00000000e+00, 4.21999979e+00],\n",
       "       [3.47000000e-01, 1.75499992e+01, 0.00000000e+00, 4.40999985e+00],\n",
       "       [3.76000000e-01, 1.71100006e+01, 0.00000000e+00, 4.51999998e+00],\n",
       "       [3.42000000e-01, 1.65900002e+01, 0.00000000e+00, 4.57999992e+00]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################# For Jerome - ignore the rest ############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = pv_weather_df.rename(columns={'forecast dt iso': 'date_of_forcast',\n",
    "                                              'slice dt iso': 'forcasting_date_range',\n",
    "                                              'fr_rain': 'freezing_rain_vol',\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = pd.merge(pv_weather_df, pv_df, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = '1980-01-01 00:00:00'\n",
    "max_date = '2019-12-31 23:00:00'\n",
    "train = pv_df[pv_df['utc_time'] <= max_date]\n",
    "test = pv_df[pv_df['utc_time'] > max_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['electricity']]\n",
    "test = test[['electricity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'electricity'\n",
    "fold_length = 43800             # 5 years\n",
    "fold_stride = 43800             # 5 years\n",
    "train_test_ratio = 0.8          # 5 yrs/6 yrs\n",
    "input_length = 48               # number of obsevations per one sequence\n",
    "output_length = 24              # Day-ahead predictions\n",
    "n_seq_train = 500               # number_of_sequences_train\n",
    "n_seq_val = 100                 # number_of_sequences_test\n",
    "n_unit = 48                     # number of hidden units\n",
    "learning_rate = 0.02\n",
    "patience = 5\n",
    "epochs = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_baseline_and_lstm():\n",
    "\n",
    "    list_of_mae_baseline_model = []\n",
    "    list_of_mae_recurrent_model = []\n",
    "\n",
    "    # 0 - Creating folds\n",
    "    # =========================================\n",
    "    folds = get_folds(train, fold_length, fold_stride)\n",
    "\n",
    "    for fold_id, fold in enumerate(folds):\n",
    "\n",
    "        # 1 - Train/Test split the current fold\n",
    "        # =========================================\n",
    "        (fold_train, fold_val) = train_test_split(fold, train_test_ratio, input_length)\n",
    "\n",
    "        X_train, y_train = get_X_y_seq(fold_train, n_seq_train, input_length, output_length, gap_hours=12)\n",
    "        X_val, y_val = get_X_y_seq(fold_val, n_seq_val, input_length, output_length, gap_hours=12)\n",
    "\n",
    "        # 2 - Modelling\n",
    "        # =========================================\n",
    "\n",
    "        ##### Baseline Model\n",
    "        baseline_model = init_baseline_keras()\n",
    "        mae_baseline = baseline_model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "        list_of_mae_baseline_model.append(mae_baseline)\n",
    "        print(\"-\"*50)\n",
    "        print(f\"MAE baseline fold n°{fold_id} = {round(mae_baseline, 2)}\")\n",
    "\n",
    "        ##### LSTM Model\n",
    "        model = initialize_model(X_train, y_train, n_unit=n_unit)\n",
    "        model = compile_model(model, learning_rate=learning_rate)\n",
    "        model, history = train_model(model,\n",
    "                                     X_train,\n",
    "                                     y_train,\n",
    "                                     validation_split = 0.2,\n",
    "                                     batch_size = batch_size,\n",
    "                                     epochs = epochs)\n",
    "\n",
    "        # Create a figure and axes object with 1 row and 2 columns\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "        # Plot training & validation MAE values\n",
    "        axes[0].plot(history.history['mae'])\n",
    "        axes[0].plot(history.history['val_mae'])\n",
    "        axes[0].set_title('Model MAE')\n",
    "        axes[0].set_ylabel('MAE')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        axes[1].plot(history.history['loss'])\n",
    "        axes[1].plot(history.history['val_loss'])\n",
    "        axes[1].set_title('Model Loss')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "        res = model.evaluate(X_val, y_val, verbose=0)    # evaluating LSTM (metric)\n",
    "        mae_lstm = res[1]\n",
    "        list_of_mae_recurrent_model.append(mae_lstm)\n",
    "        print(f\"MAE LSTM fold n°{fold_id} = {round(mae_lstm, 2)}\")\n",
    "\n",
    "        ##### Comparison LSTM vs Baseline for the current fold\n",
    "        print(f\"improvement over baseline: {round((1 - (mae_lstm/mae_baseline))*100,2)} % \\n\")\n",
    "\n",
    "    return list_of_mae_baseline_model, list_of_mae_recurrent_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_baselines, mae_lstms = cross_validate_baseline_and_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"average percentage improvement over baseline = {round(np.mean(1 - (np.array(mae_lstms)/np.array(mae_baselines))),2)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y_seq(train, number_of_sequences=10000, input_length=48, output_length=24, gap_hours=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(X_train, y_train, n_unit=n_unit)\n",
    "model = compile_model(model, learning_rate=learning_rate)\n",
    "model, history = train_model(model,\n",
    "                                X_train,\n",
    "                                y_train,\n",
    "                                validation_split = 0.1,\n",
    "                                batch_size = batch_size,\n",
    "                                epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power.params import *\n",
    "from power.ml_ops.data import get_data_with_cache\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "input_pred = '2021-05-08 12:00:00'\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "    ORDER BY utc_time\n",
    "\"\"\"\n",
    "\n",
    "data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "data_processed = get_data_with_cache(\n",
    "    gcp_project=GCP_PROJECT,\n",
    "    query=query,\n",
    "    cache_path=data_processed_cache_path,\n",
    "    data_has_header=True\n",
    ")\n",
    "\n",
    "X_pred = data_processed[data_processed['utc_time'] < input_pred][-48:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = X_pred[['electricity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = X_pred.to_numpy()\n",
    "X_pred_tf = tf.convert_to_tensor(X_pred)\n",
    "X_pred_tf = tf.expand_dims(X_pred_tf, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_pred_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
