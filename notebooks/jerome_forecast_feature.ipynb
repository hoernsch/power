{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 09:48:56.178511: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-15 09:48:57.179164: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-15 09:48:57.184998: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-15 09:49:11.877473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "from colorama import Fore, Style\n",
    "\n",
    "from power.params import *\n",
    "from power.ml_ops.data import get_data_with_cache, get_forecast_data, clean_forecast_data\n",
    "from power.ml_ops.cross_val import get_folds, train_test_split, get_X_y_seq\n",
    "from power.ml_ops.model import init_baseline_keras, compile_model, initialize_model, train_model\n",
    "from power.utils import compress\n",
    "\n",
    "# tensforflow\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../raw_data/history_forecast_bulk_20171007_20240312.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Reduces size of dataframe by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum()/ 1024\n",
    "    print(\"new dataframe size: \", round(input_size,2), 'kB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    for type in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=type))\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=type)\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new dataframe size: \", round(out_size / 1024,2), \" kB\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = compress(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_data.copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['forecast dt iso', 'slice dt iso', 'temperature', 'dew_point', 'pressure',\n",
    "       'ground_pressure', 'humidity', 'clouds', 'wind_speed', 'wind_deg',\n",
    "       'rain', 'snow', 'ice', 'fr_rain', 'convective', 'snow_depth',\n",
    "       'accumulated', 'hours', 'rate', 'probability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['forecast dt iso'] = df['forecast dt iso'].str.replace('+0000 UTC', '')\n",
    "df['slice dt iso'] = df['slice dt iso'].str.replace('+0000 UTC', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['forecast dt iso'].str.contains('12:00:00')]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['forecast dt iso'] = pd.to_datetime(df['forecast dt iso'])\n",
    "df['slice dt iso'] = pd.to_datetime(df['slice dt iso'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_dates = df['forecast dt iso'].unique()\n",
    "df_unique_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['forecast dt iso'] == df_unique_dates[0]) & (df['slice dt iso'].between(df_unique_dates[0] + timedelta(days=1) - timedelta(hours=12), df_unique_dates[0] + timedelta(days=1) + timedelta(hours=11)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revised = []\n",
    "for date in df_unique_dates:\n",
    "    data = df[(df['forecast dt iso'] == date) & (df['slice dt iso'].between(date + timedelta(days=1) - timedelta(hours=12), date + timedelta(days=2) + timedelta(hours=11)))]\n",
    "    df_revised.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revised_ordered = pd.concat(df_revised, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_revised_ordered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = df_revised_ordered[df_revised_ordered['slice dt iso'] <= '2022-12-31 23:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_forecast_data(forecast_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Initial has 3.3 M entries (everyday: 4 forecasts of 16 days ahead)\n",
    "    Cleaning it to: - 1 forecast perday (at 12:00)\n",
    "                    - 48 hours a day\n",
    "                    - right now hardcoded to match last forecast day with\n",
    "                     last day of PV data\n",
    "    \"\"\"\n",
    "    df = compress(forecast_df)\n",
    "\n",
    "    # get only 1 forecast per day and deal with uncommon UTC format\n",
    "    df['forecast dt iso'] = df['forecast dt iso'].str.replace('+0000 UTC', '')\n",
    "    df['slice dt iso'] = df['slice dt iso'].str.replace('+0000 UTC', '')\n",
    "\n",
    "    df = df[df['forecast dt iso'].str.contains('12:00:00')]\n",
    "\n",
    "    df['forecast dt iso'] = pd.to_datetime(df['forecast dt iso'])\n",
    "    df['slice dt iso'] = pd.to_datetime(df['slice dt iso'])\n",
    "\n",
    "    df_unique_dates = df['forecast dt iso'].unique()\n",
    "\n",
    "    # reduce to 48h of weather forecast (from 00:00 to 23:00 each day)\n",
    "    df_revised = []\n",
    "    for date in df_unique_dates:\n",
    "        data = df[(df['forecast dt iso'] == date) & (df['slice dt iso'].between(date + timedelta(days=1) - timedelta(hours=12), date + timedelta(days=2) + timedelta(hours=11)))]\n",
    "        df_revised.append(data)\n",
    "\n",
    "    df_revised_ordered = pd.concat(df_revised, ignore_index=True)\n",
    "\n",
    "    # hard code the end date to match wiht PV data\n",
    "    processed_df = df_revised_ordered[df_revised_ordered['slice dt iso'] <= '2022-12-31 23:00:00']\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = get_forecast_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = clean_forecast_data(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df.to_csv('../raw_data/weather_forecast_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = pd.read_csv('../raw_data/weather_forecast_processed.csv')\n",
    "pv_weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pv_weather_df.copy()\n",
    "data.rename(columns={'forecast dt iso':'utc_time', 'slice dt iso':'prediction_utc_time'}, inplace=True)\n",
    "data['utc_time'] = pd.to_datetime(data['utc_time'])\n",
    "data['prediction_utc_time'] = pd.to_datetime(data['prediction_utc_time'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_date ='2020-12-06'\n",
    "input_datetime = datetime.strptime(input_date, '%Y-%m-%d')\n",
    "data[data.utc_time.dt.date == (input_datetime.date())].iloc[:,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_date ='2020-06-30'\n",
    "input_datetime = datetime.strptime(input_date, '%Y-%m-%d')\n",
    "\n",
    "df_forecast_day_before_input_date = data[data.utc_time.dt.date == (input_datetime.date() - timedelta(days=1))].iloc[-24:,:]\n",
    "df_forecast_input_date = data[data.utc_time.dt.date == input_datetime.date()].iloc[:24,:]\n",
    "df_forecast = pd.concat([df_forecast_day_before_input_date, df_forecast_input_date], axis=0).reset_index(drop=True)\n",
    "\n",
    "features = ['temperature', 'clouds', 'wind_deg', 'rain', 'snow',]\n",
    "X=df_forecast[features]\n",
    "fig, ax = plt.subplots(nrows=5, ncols=2, figsize= (16,9))\n",
    "for idx, feature in enumerate(features):\n",
    "    sns.boxplot(data=X, x=feature, ax=ax[idx,0], legend='auto')\n",
    "    sns.histplot(data=X, x=feature, ax=ax[idx,1], bins =5)\n",
    "# df_forecast.iloc[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast.utc_time.nunique(), df_forecast.prediction_utc_time.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_forecast.utc_time.value_counts(), df_forecast.prediction_utc_time.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler= MinMaxScaler()\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=5, ncols=2, figsize= (16,9))\n",
    "for idx, feature in enumerate(features[-5:]):\n",
    "    sns.boxplot(data=X, x=feature, ax=ax[idx,0], legend='auto')\n",
    "    sns.histplot(data=X, x=feature, ax=ax[idx,1], bins =5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSEUDO-CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when getting the 10_000 sequences, the dt index should be used to extract 48 observations and x features\n",
    "\n",
    "rnow an ouput of shape (48,1)\n",
    "\n",
    "should become an output (48, 1 + x)\n",
    "\n",
    "call a data module.function to process the weather forecast features\n",
    "\n",
    "concat the data somehow: np.concat? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test get weather feature function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m\n",
      "⭐️ Use case: train\u001b[0m\n",
      "\u001b[34m\n",
      "Loading preprocessed validation data...\u001b[0m\n",
      "\u001b[34m\n",
      "Load data from local CSV...\u001b[0m\n",
      "✅ Data loaded, with shape (376944, 3)\n",
      "\u001b[34m\n",
      "Load data from local CSV...\u001b[0m\n",
      "✅ Data loaded, with shape (91704, 24)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((100, 48, 5), (100, 24, 1))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(\n",
    "        min_date = '2017-10-07 00:00:00',\n",
    "        max_date = '2019-12-31 23:00:00',\n",
    "        split_ratio: float = 0.02, # 0.02 represents ~ 1 month of validation data on a 2009-2015 train set\n",
    "        learning_rate=0.02,\n",
    "        batch_size = 32,\n",
    "        patience = 5\n",
    "    ) -> float:\n",
    "\n",
    "    \"\"\"\n",
    "    - Download processed data from your BQ table (or from cache if it exists)\n",
    "    - Train on the preprocessed dataset (which should be ordered by date)\n",
    "    - Store training results and model weights\n",
    "\n",
    "    Return val_mae as a float\n",
    "    \"\"\"\n",
    "\n",
    "    print(Fore.MAGENTA + \"\\n⭐️ Use case: train\" + Style.RESET_ALL)\n",
    "    print(Fore.BLUE + \"\\nLoading preprocessed validation data...\" + Style.RESET_ALL)\n",
    "\n",
    "\n",
    "    # --First-- Load processed PV data using `get_data_with_cache` in chronological order\n",
    "    query_pv = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "        ORDER BY utc_time\n",
    "    \"\"\"\n",
    "\n",
    "    data_processed_pv_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "    data_processed_pv = get_data_with_cache(\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        query=query_pv,\n",
    "        cache_path=data_processed_pv_cache_path,\n",
    "        data_has_header=True\n",
    "    )\n",
    "\n",
    "    # --Second-- Load processed Weather Forecast data in chronological order\n",
    "    query_forecast = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_weather_forecast\n",
    "        ORDER BY forecast_dt_unixtime, slice_dt_unixtime\n",
    "    \"\"\"\n",
    "\n",
    "    data_processed_forecast_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_weather_forecast.csv\")\n",
    "    data_processed_forecast = get_data_with_cache(\n",
    "        gcp_project=GCP_PROJECT,\n",
    "        query=query_forecast,\n",
    "        cache_path=data_processed_forecast_cache_path,\n",
    "        data_has_header=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # the processed PV data from bq needs to be converted to datetime object\n",
    "    data_processed_pv.utc_time = pd.to_datetime(data_processed_pv.utc_time,utc=True)\n",
    "\n",
    "    if data_processed_pv.shape[0] < 240:\n",
    "        print(\"❌ Not enough processed data retrieved to train on\")\n",
    "        return None\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_pv = data_processed_pv[(data_processed_pv['utc_time'] > min_date) \\\n",
    "                                 & (data_processed_pv['utc_time'] < max_date)]\n",
    "\n",
    "    if data_processed_forecast.shape[0] < 240:\n",
    "        print(\"❌ Not enough processed data retrieved to train on\")\n",
    "        return None\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_forecast = data_processed_forecast\n",
    "\n",
    "    X_train, y_train = get_X_y_seq(train_pv,\n",
    "                                   train_forecast,\n",
    "                                   number_of_sequences=100,\n",
    "                                   input_length=48,\n",
    "                                   output_length=24,\n",
    "                                   gap_hours=12)\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "X_train, y_train = train()\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.000</td>\n",
       "      <td>15.230000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000</td>\n",
       "      <td>14.560000</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.000</td>\n",
       "      <td>13.810000</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.000</td>\n",
       "      <td>13.340000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.007</td>\n",
       "      <td>13.440000</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.038</td>\n",
       "      <td>14.010000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.106</td>\n",
       "      <td>14.890000</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.225</td>\n",
       "      <td>15.910000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.307</td>\n",
       "      <td>17.030001</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.461</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.594</td>\n",
       "      <td>19.530001</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.547</td>\n",
       "      <td>20.770000</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.567</td>\n",
       "      <td>21.850000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.472</td>\n",
       "      <td>22.660000</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.313</td>\n",
       "      <td>23.170000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.216</td>\n",
       "      <td>23.379999</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.177</td>\n",
       "      <td>23.270000</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.114</td>\n",
       "      <td>22.840000</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.054</td>\n",
       "      <td>22.080000</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.014</td>\n",
       "      <td>21.020000</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.000</td>\n",
       "      <td>19.870001</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.000</td>\n",
       "      <td>18.900000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000</td>\n",
       "      <td>18.290001</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.000</td>\n",
       "      <td>17.940001</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0          1     2    3     4\n",
       "24  0.000  15.230000  99.0  0.0  3.27\n",
       "25  0.000  14.560000  88.0  0.0  2.81\n",
       "26  0.000  13.810000  73.0  0.0  2.36\n",
       "27  0.000  13.340000  57.0  0.0  2.07\n",
       "28  0.007  13.440000  42.0  0.0  2.06\n",
       "29  0.038  14.010000  32.0  0.0  2.26\n",
       "30  0.106  14.890000  28.0  0.0  2.54\n",
       "31  0.225  15.910000  32.0  0.0  2.78\n",
       "32  0.307  17.030001  40.0  0.0  2.93\n",
       "33  0.461  18.250000  48.0  0.0  2.97\n",
       "34  0.594  19.530001  51.0  0.0  2.87\n",
       "35  0.547  20.770000  49.0  0.0  2.70\n",
       "36  0.567  21.850000  43.0  0.0  2.54\n",
       "37  0.472  22.660000  34.0  0.0  2.48\n",
       "38  0.313  23.170000  26.0  0.0  2.55\n",
       "39  0.216  23.379999  22.0  0.0  2.78\n",
       "40  0.177  23.270000  26.0  0.0  3.17\n",
       "41  0.114  22.840000  36.0  0.0  3.49\n",
       "42  0.054  22.080000  48.0  0.0  3.47\n",
       "43  0.014  21.020000  59.0  0.0  2.98\n",
       "44  0.000  19.870001  68.0  0.0  2.34\n",
       "45  0.000  18.900000  75.0  0.0  2.02\n",
       "46  0.000  18.290001  80.0  0.0  2.33\n",
       "47  0.000  17.940001  84.0  0.0  2.92"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(0, X_train.shape[0])\n",
    "pd.DataFrame(X_train[idx]).iloc[-24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = pd.read_csv('../raw_data/weather_forecast_processed.csv')\n",
    "\n",
    "def compress(df, **kwargs):\n",
    "    \"\"\"\n",
    "    Reduces size of dataframe by downcasting numerical columns\n",
    "    \"\"\"\n",
    "    input_size = df.memory_usage(index=True).sum()/ 1024\n",
    "    print(\"new dataframe size: \", round(input_size,2), 'kB')\n",
    "\n",
    "    in_size = df.memory_usage(index=True).sum()\n",
    "    for type in [\"float\", \"integer\"]:\n",
    "        l_cols = list(df.select_dtypes(include=type))\n",
    "        for col in l_cols:\n",
    "            df[col] = pd.to_numeric(df[col], downcast=type)\n",
    "    out_size = df.memory_usage(index=True).sum()\n",
    "    ratio = (1 - round(out_size / in_size, 2)) * 100\n",
    "\n",
    "    print(\"optimized size by {} %\".format(round(ratio,2)))\n",
    "    print(\"new dataframe size: \", round(out_size / 1024,2), \" kB\")\n",
    "\n",
    "    return df\n",
    "\n",
    "pv_weather_df = compress(pv_weather_df)\n",
    "\n",
    "def get_weather_forecast_features(forecast: pd.DataFrame, input_date: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    returns the weather forecast data from historical weather forecast in Tempelhof\n",
    "    input: - a processed forecast dataframe of shape (91704,21)\n",
    "           - an input date (str: YYYY-MM-DD)\n",
    "    output: a dataframe of shape (48, 21)\n",
    "            -> first 24 rows: hourly (from 00:00 to 23:00) weather forecast\n",
    "               of input_date +1 forecast on input_date -1 (at 12:00)\n",
    "            -> second 24 rows: hourly (from 00:00 to 23:00) weather forecast\n",
    "               of input_date +1 forecast on input_date (at 12:00)\n",
    "    \"\"\"\n",
    "    forecast.rename(columns={'forecast_dt_iso':'utc_time',\n",
    "                        'slice_dt_iso':'prediction_utc_time'},\n",
    "                        inplace=True)\n",
    "    forecast['utc_time'] = pd.to_datetime(forecast['utc_time'])\n",
    "    forecast['prediction_utc_time'] = pd.to_datetime(forecast['prediction_utc_time'])\n",
    "\n",
    "    input_datetime = datetime.strptime(input_date, '%Y-%m-%d')\n",
    "\n",
    "    forecast_day_before_input_date = forecast[forecast.utc_time.dt.date == (input_datetime.date() - timedelta(days=1))].iloc[-24:,:]\n",
    "    forecast_input_date = forecast[forecast.utc_time.dt.date == input_datetime.date()].iloc[:24,:]\n",
    "    df_forecast = pd.concat([forecast_day_before_input_date,\n",
    "                             forecast_input_date], axis=0).reset_index(drop=True)\n",
    "    return df_forecast\n",
    "\n",
    "test_df = get_weather_forecast_features(pv_weather_df, '2020-06-30')\n",
    "test_df.iloc[:,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try editing sequence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xi_yi(\n",
    "    pv_fold:pd.DataFrame,\n",
    "    forecast_fold:pd.DataFrame,\n",
    "    input_length:int,       # 48\n",
    "    output_length:int,      # 24\n",
    "    gap_hours):\n",
    "    '''\n",
    "    - given a fold, it returns one sequence (X_i, y_i)\n",
    "    - with the starting point of the sequence being chosen at random\n",
    "    - TARGET is the variable(s) we want to predict (name of the column(s))\n",
    "    '''\n",
    "    TARGET = 'electricity'\n",
    "    first_possible_start = 0\n",
    "    last_possible_start = len(pv_fold) - (input_length + gap_hours + output_length) + 1\n",
    "\n",
    "    random_start = np.random.randint(first_possible_start, last_possible_start)\n",
    "\n",
    "    # input_start & input_end are the indexes of the 48h (training length) of training data\n",
    "    # thus for weather forecast data from input_end should be used to add\n",
    "    # the weather forecast features\n",
    "    input_start = random_start\n",
    "    input_end = random_start + input_length\n",
    "    # here we extract the forecast date and hour\n",
    "    forecast_date = pv_fold.iloc[input_end]['utc_time'].strftime('%Y-%m-%d')\n",
    "    forecast_hour = pv_fold.iloc[input_end]['utc_time'].hour\n",
    "\n",
    "    target_start = input_end + gap_hours\n",
    "    target_end = target_start + output_length\n",
    "\n",
    "    # first we parse the electricity feature\n",
    "    # need to reset index only for X_i in order to be able to concat with X_weather later on\n",
    "    X_i = pv_fold.iloc[input_start:input_end].reset_index()\n",
    "    y_i = pv_fold.iloc[target_start:target_end][[TARGET]]    # creates a pd.DataFrame for the target y\n",
    "\n",
    "    # then we parse/create the weather forecast features\n",
    "    #TODO : proper data\n",
    "    X_weather = get_weather_forecast_features(forecast_fold, forecast_date)\n",
    "\n",
    "    #\n",
    "    features = ['temperature', 'clouds', 'wind_speed']\n",
    "    to_concat = [X_i['electricity'], X_weather[features]]\n",
    "    X_i = pd.concat(to_concat, axis=1)\n",
    "    return (X_i, y_i)\n",
    "\n",
    "min_date = '2017-10-07 12:00:00'\n",
    "max_date = '2019-12-31 23:00:00'\n",
    "query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "        ORDER BY utc_time\n",
    "    \"\"\"\n",
    "\n",
    "data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "data_processed = get_data_with_cache(\n",
    "    gcp_project=GCP_PROJECT,\n",
    "    query=query,\n",
    "    cache_path=data_processed_cache_path,\n",
    "    data_has_header=True\n",
    ")\n",
    "\n",
    "# the processed data from bq needs to be converted to datetime object\n",
    "data_processed.utc_time = pd.to_datetime(data_processed.utc_time,utc=True)\n",
    "\n",
    "if data_processed.shape[0] < 240:\n",
    "    print(\"❌ Not enough processed data retrieved to train on\")\n",
    "    # return None\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train = data_processed[(data_processed['utc_time'] > min_date) \\\n",
    "                        & (data_processed['utc_time'] < max_date)]\n",
    "# train = data_processed[data_processed['utc_time'] < max_date]\n",
    "\n",
    "# train = train[['electricity']]\n",
    "\n",
    "fold = train.copy()\n",
    "number_of_sequences=100\n",
    "input_length=48\n",
    "output_length=24\n",
    "gap_hours=12\n",
    "\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(number_of_sequences):\n",
    "    (Xi, yi) = get_Xi_yi(fold, input_length, output_length, gap_hours)\n",
    "    X.append(Xi)\n",
    "    y.append(yi)\n",
    "\n",
    "X_train, y_train = np.array(X), np.array(y)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1:], y_train.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Jerome - ignore the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_weather_df = pv_weather_df.rename(columns={'forecast dt iso': 'date_of_forcast',\n",
    "                                              'slice dt iso': 'forcasting_date_range',\n",
    "                                              'fr_rain': 'freezing_rain_vol',\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_df = pd.merge(pv_weather_df, pv_df, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = '1980-01-01 00:00:00'\n",
    "max_date = '2019-12-31 23:00:00'\n",
    "train = pv_df[pv_df['utc_time'] <= max_date]\n",
    "test = pv_df[pv_df['utc_time'] > max_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['electricity']]\n",
    "test = test[['electricity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = 'electricity'\n",
    "fold_length = 43800             # 5 years\n",
    "fold_stride = 43800             # 5 years\n",
    "train_test_ratio = 0.8          # 5 yrs/6 yrs\n",
    "input_length = 48               # number of obsevations per one sequence\n",
    "output_length = 24              # Day-ahead predictions\n",
    "n_seq_train = 500               # number_of_sequences_train\n",
    "n_seq_val = 100                 # number_of_sequences_test\n",
    "n_unit = 48                     # number of hidden units\n",
    "learning_rate = 0.02\n",
    "patience = 5\n",
    "epochs = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_baseline_and_lstm():\n",
    "\n",
    "    list_of_mae_baseline_model = []\n",
    "    list_of_mae_recurrent_model = []\n",
    "\n",
    "    # 0 - Creating folds\n",
    "    # =========================================\n",
    "    folds = get_folds(train, fold_length, fold_stride)\n",
    "\n",
    "    for fold_id, fold in enumerate(folds):\n",
    "\n",
    "        # 1 - Train/Test split the current fold\n",
    "        # =========================================\n",
    "        (fold_train, fold_val) = train_test_split(fold, train_test_ratio, input_length)\n",
    "\n",
    "        X_train, y_train = get_X_y_seq(fold_train, n_seq_train, input_length, output_length, gap_hours=12)\n",
    "        X_val, y_val = get_X_y_seq(fold_val, n_seq_val, input_length, output_length, gap_hours=12)\n",
    "\n",
    "        # 2 - Modelling\n",
    "        # =========================================\n",
    "\n",
    "        ##### Baseline Model\n",
    "        baseline_model = init_baseline_keras()\n",
    "        mae_baseline = baseline_model.evaluate(X_val, y_val, verbose=0)[1]\n",
    "        list_of_mae_baseline_model.append(mae_baseline)\n",
    "        print(\"-\"*50)\n",
    "        print(f\"MAE baseline fold n°{fold_id} = {round(mae_baseline, 2)}\")\n",
    "\n",
    "        ##### LSTM Model\n",
    "        model = initialize_model(X_train, y_train, n_unit=n_unit)\n",
    "        model = compile_model(model, learning_rate=learning_rate)\n",
    "        model, history = train_model(model,\n",
    "                                     X_train,\n",
    "                                     y_train,\n",
    "                                     validation_split = 0.2,\n",
    "                                     batch_size = batch_size,\n",
    "                                     epochs = epochs)\n",
    "\n",
    "        # Create a figure and axes object with 1 row and 2 columns\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
    "\n",
    "        # Plot training & validation MAE values\n",
    "        axes[0].plot(history.history['mae'])\n",
    "        axes[0].plot(history.history['val_mae'])\n",
    "        axes[0].set_title('Model MAE')\n",
    "        axes[0].set_ylabel('MAE')\n",
    "        axes[0].set_xlabel('Epoch')\n",
    "        axes[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        axes[1].plot(history.history['loss'])\n",
    "        axes[1].plot(history.history['val_loss'])\n",
    "        axes[1].set_title('Model Loss')\n",
    "        axes[1].set_ylabel('Loss')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "        # Adjust layout to prevent overlap\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "        res = model.evaluate(X_val, y_val, verbose=0)    # evaluating LSTM (metric)\n",
    "        mae_lstm = res[1]\n",
    "        list_of_mae_recurrent_model.append(mae_lstm)\n",
    "        print(f\"MAE LSTM fold n°{fold_id} = {round(mae_lstm, 2)}\")\n",
    "\n",
    "        ##### Comparison LSTM vs Baseline for the current fold\n",
    "        print(f\"improvement over baseline: {round((1 - (mae_lstm/mae_baseline))*100,2)} % \\n\")\n",
    "\n",
    "    return list_of_mae_baseline_model, list_of_mae_recurrent_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_baselines, mae_lstms = cross_validate_baseline_and_lstm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"average percentage improvement over baseline = {round(np.mean(1 - (np.array(mae_lstms)/np.array(mae_baselines))),2)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_X_y_seq(train, number_of_sequences=10000, input_length=48, output_length=24, gap_hours=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model(X_train, y_train, n_unit=n_unit)\n",
    "model = compile_model(model, learning_rate=learning_rate)\n",
    "model, history = train_model(model,\n",
    "                                X_train,\n",
    "                                y_train,\n",
    "                                validation_split = 0.1,\n",
    "                                batch_size = batch_size,\n",
    "                                epochs = epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power.params import *\n",
    "from power.ml_ops.data import get_data_with_cache\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "\n",
    "input_pred = '2021-05-08 12:00:00'\n",
    "\n",
    "query = f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {GCP_PROJECT}.{BQ_DATASET}.processed_pv\n",
    "    ORDER BY utc_time\n",
    "\"\"\"\n",
    "\n",
    "data_processed_cache_path = Path(LOCAL_DATA_PATH).joinpath(\"processed\", f\"processed_pv.csv\")\n",
    "data_processed = get_data_with_cache(\n",
    "    gcp_project=GCP_PROJECT,\n",
    "    query=query,\n",
    "    cache_path=data_processed_cache_path,\n",
    "    data_has_header=True\n",
    ")\n",
    "\n",
    "X_pred = data_processed[data_processed['utc_time'] < input_pred][-48:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = X_pred[['electricity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = X_pred.to_numpy()\n",
    "X_pred_tf = tf.convert_to_tensor(X_pred)\n",
    "X_pred_tf = tf.expand_dims(X_pred_tf, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_pred_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
